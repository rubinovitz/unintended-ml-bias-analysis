{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Toxicity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a model to detect toxicity in online comments. It uses a CNN architecture for text classification trained on the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) and pre-trained GloVe embeddings which can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "This model is a modification of [example code](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py) found in the [Keras Github repository](https://github.com/fchollet/keras) and released under an [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE). For further details of this license, find it [online](https://github.com/fchollet/keras/blob/master/LICENSE) or in this repository in the file KERAS_LICENSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "(TODO: nthain) - Move to README\n",
    "\n",
    "Prior to running the notebook, you must:\n",
    "\n",
    "* Download the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973)\n",
    "* Download pre-trained [GloVe embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "* (optional) To skip the training step, you will need to download a model and tokenizer file. We are looking into the appropriate means for distributing these (sometimes large) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from model_tool import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_tool import (DEFAULT_EMBEDDINGS_PATH,\n",
    "DEFAULT_MODEL_DIR, DEFAULT_HPARAMS)\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Multiply\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "\n",
    "\n",
    "class AttentionToxModel(ToxModel):\n",
    "    def __init__(self,\n",
    "                 model_name=None,\n",
    "                 model_dir=DEFAULT_MODEL_DIR,\n",
    "                 embeddings_path=DEFAULT_EMBEDDINGS_PATH,\n",
    "                 hparams=None):\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.hparams = DEFAULT_HPARAMS.copy()\n",
    "        self.embeddings_path = embeddings_path\n",
    "        if hparams:\n",
    "            self.update_hparams(hparams)\n",
    "        if model_name:\n",
    "            self.load_model_from_name(model_name)\n",
    "            self.load_probs_model(model_name)\n",
    "        self.print_hparams()\n",
    "\n",
    "    def load_probs_model(self, model_name):\n",
    "        probs_model_name = model_name + \"_probs\"\n",
    "        self.probs_model = load_model(\n",
    "                os.path.join(\n",
    "                    self.model_dir, '%s_model.h5' % probs_model_name))\n",
    "\n",
    "    def save_prob_model(self):\n",
    "        self.probs_model_name = self.model_name + \"probs\"\n",
    "\n",
    "    def build_dense_attention_layer(self, input_tensor):\n",
    "        # softmax\n",
    "        attention_probs = Dense(self.hparams['max_sequence_length'],\n",
    "                                activation='softmax',\n",
    "                                name='attention_vec')(input_tensor)\n",
    "        # context vector\n",
    "        attention_mul = Multiply()([input_tensor, attention_probs])\n",
    "        return {'attention_probs': attention_probs,\n",
    "                'attention_preds': attention_mul}\n",
    "\n",
    "    def train(\n",
    "                self,\n",
    "                training_data_path,\n",
    "                validation_data_path, text_column,\n",
    "                label_column, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.save_hparams(model_name)\n",
    "\n",
    "        train_data = pd.read_csv(training_data_path)\n",
    "        valid_data = pd.read_csv(validation_data_path)\n",
    "\n",
    "        print('Fitting tokenizer...')\n",
    "        self.fit_and_save_tokenizer(train_data[text_column])\n",
    "        print('Tokenizer fitted!')\n",
    "\n",
    "        print('Preparing data...')\n",
    "        train_text, train_labels = (self.prep_text(train_data[text_column]),\n",
    "                                    to_categorical(train_data[label_column]))\n",
    "        valid_text, valid_labels = (self.prep_text(valid_data[text_column]),\n",
    "                                    to_categorical(valid_data[label_column]))\n",
    "        print('Data prepared!')\n",
    "\n",
    "        print('Loading embeddings...')\n",
    "        self.load_embeddings()\n",
    "        print('Embeddings loaded!')\n",
    "\n",
    "        print('Building model graph...')\n",
    "        self.build_model()\n",
    "        print('Training model...')\n",
    "\n",
    "        preds_save_path = os.path.join(\n",
    "                            self.model_dir, '%s_model.h5' % self.model_name)\n",
    "        probs_save_path = os.path.join(\n",
    "                            self.model_dir, '%s_probs_model.h5'\n",
    "                            % self.model_name)\n",
    "        preds_callbacks = [ModelCheckpoint(\n",
    "                            preds_save_path,\n",
    "                            save_best_only=True,\n",
    "                            verbose=self.hparams['verbose']), tensorboard]\n",
    "        probs_callbacks = [ModelCheckpoint(\n",
    "                            probs_save_path,\n",
    "                            save_best_only=True,\n",
    "                            verbose=self.hparams['verbose']),tensorboard]\n",
    "\n",
    "        if self.hparams['stop_early']:\n",
    "            early_stop = EarlyStopping(\n",
    "                            min_delta=self.hparams['es_min_delta'],\n",
    "                            monitor='val_loss',\n",
    "                            patience=self.hparams['es_patience'],\n",
    "                            verbose=self.hparams['verbose'], mode='auto')\n",
    "            probs_callbacks.append(early_stop)\n",
    "            preds_callbacks.append(early_stop)\n",
    "\n",
    "        self.model.fit(train_text,\n",
    "                       train_labels,\n",
    "                       batch_size=self.hparams['batch_size'],\n",
    "                       epochs=self.hparams['epochs'],\n",
    "                       validation_data=(valid_text, valid_labels),\n",
    "                       callbacks=preds_callbacks,\n",
    "                       verbose=2)\n",
    "\n",
    "        print('Model trained!')\n",
    "        print('Best model saved to {}'.format(preds_save_path))\n",
    "        print('Fitting probs model')\n",
    "\n",
    "        self.probs_model.fit(\n",
    "                    train_text,\n",
    "                    train_labels,\n",
    "                    batch_size=self.hparams['batch_size'],\n",
    "                    epochs=self.hparams['epochs'],\n",
    "                    validation_data=(valid_text, valid_labels),\n",
    "                    callbacks=probs_callbacks,\n",
    "                    verbose=2)\n",
    "        self.probs_model = load_model(probs_save_path)\n",
    "        print('Loading best model from checkpoint...')\n",
    "        self.model = load_model(preds_save_path)\n",
    "        print('Model loaded!')\n",
    "\n",
    "    def build_model(self):\n",
    "        print('print inside build model')\n",
    "        sequence_input = Input(\n",
    "                            shape=(self.hparams['max_sequence_length'],),\n",
    "                            dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "                            len(self.tokenizer.word_index) + 1,\n",
    "                            self.hparams['embedding_dim'],\n",
    "                            weights=[self.embedding_matrix],\n",
    "                            input_length=self.hparams['max_sequence_length'],\n",
    "                            trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = embedded_sequences\n",
    "        for filter_size, kernel_size, pool_size in zip(\n",
    "                self.hparams['cnn_filter_sizes'],\n",
    "                self.hparams['cnn_kernel_sizes'],\n",
    "                self.hparams['cnn_pooling_sizes']):\n",
    "            x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(self.hparams['dropout_rate'], name=\"Dropout\")(x)\n",
    "        x = Dense(250, activation='relu', name=\"Dense_RELU\")(x)\n",
    "\n",
    "        # build prediction model\n",
    "        attention_dict = self.build_dense_attention_layer(x)\n",
    "        preds = attention_dict['attention_preds']\n",
    "        preds = Dense(2, name=\"preds_dense\", activation='softmax')(preds)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        self.model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=rmsprop,\n",
    "                metrics=['acc'])\n",
    "\n",
    "        # now make probs model\n",
    "        probs = attention_dict['attention_probs']\n",
    "        probs = Dense(2, name='probs_dense')(probs)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.probs_model = Model(sequence_input, preds)\n",
    "        self.probs_model.compile(\n",
    "                loss='mse', optimizer=rmsprop, metrics=['acc'])\n",
    "        # build probabilities model\n",
    "        self.save_prob_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/wiki_debias_random_%s.csv' % split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams = {'epochs': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.18065, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "109s - loss: 0.2349 - acc: 0.9181 - val_loss: 0.1807 - val_acc: 0.9359\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.18065 to 0.14876, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "108s - loss: 0.1624 - acc: 0.9407 - val_loss: 0.1488 - val_acc: 0.9461\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14876 to 0.13912, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "120s - loss: 0.1437 - acc: 0.9473 - val_loss: 0.1391 - val_acc: 0.9484\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13912 to 0.12939, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "109s - loss: 0.1317 - acc: 0.9521 - val_loss: 0.1294 - val_acc: 0.9522\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.12939 to 0.12599, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "105s - loss: 0.1230 - acc: 0.9543 - val_loss: 0.1260 - val_acc: 0.9540\n",
      "Epoch 6/20\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cnn_debias_random_tox_v3'\n",
    "debias_random_model = ToxModel(hparams=hparams)\n",
    "debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23986, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "105s - loss: 0.3014 - acc: 0.9066 - val_loss: 0.2399 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23986 to 0.20081, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.2208 - acc: 0.9125 - val_loss: 0.2008 - val_acc: 0.9265\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20081 to 0.17676, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1911 - acc: 0.9314 - val_loss: 0.1768 - val_acc: 0.9384\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17676 to 0.16261, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1718 - acc: 0.9386 - val_loss: 0.1626 - val_acc: 0.9396\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16261 to 0.14827, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1571 - acc: 0.9423 - val_loss: 0.1483 - val_acc: 0.9447\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14827 to 0.14028, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1470 - acc: 0.9453 - val_loss: 0.1403 - val_acc: 0.9476\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14028 to 0.13410, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1382 - acc: 0.9487 - val_loss: 0.1341 - val_acc: 0.9493\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13410 to 0.13122, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1311 - acc: 0.9501 - val_loss: 0.1312 - val_acc: 0.9519\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13122 to 0.12781, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "108s - loss: 0.1256 - acc: 0.9529 - val_loss: 0.1278 - val_acc: 0.9528\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12781 to 0.12300, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "108s - loss: 0.1204 - acc: 0.9548 - val_loss: 0.1230 - val_acc: 0.9545\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "108s - loss: 0.1162 - acc: 0.9564 - val_loss: 0.1249 - val_acc: 0.9537\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "108s - loss: 0.1119 - acc: 0.9579 - val_loss: 0.1267 - val_acc: 0.9534\n",
      "Epoch 00011: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03382, saving model to ../models/20_atn_cnn_debias_random_tox_v3_probs_model.h5\n",
      "108s - loss: 0.0304 - acc: 0.9601 - val_loss: 0.0338 - val_acc: 0.9559\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = '20_atn_cnn_debias_random_tox_v3'\n",
    "debias_random_model = AttentionToxModel(hparams=hparams)\n",
    "debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "debias_random_model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00000: val_loss improved from inf to 0.23601, saving model to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "100s - loss: 0.3048 - acc: 0.9020 - val_loss: 0.2360 - val_acc: 0.9045\n",
      "Epoch 2/4\n",
      "Epoch 00001: val_loss improved from 0.23601 to 0.19425, saving model to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "103s - loss: 0.2152 - acc: 0.9170 - val_loss: 0.1943 - val_acc: 0.9311\n",
      "Epoch 3/4\n",
      "Epoch 00002: val_loss improved from 0.19425 to 0.17229, saving model to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "104s - loss: 0.1844 - acc: 0.9343 - val_loss: 0.1723 - val_acc: 0.9377\n",
      "Epoch 4/4\n",
      "Epoch 00003: val_loss improved from 0.17229 to 0.15996, saving model to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "104s - loss: 0.1654 - acc: 0.9394 - val_loss: 0.1600 - val_acc: 0.9409\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00000: val_loss improved from inf to 0.04160, saving model to ../models/atn_cnn_wiki_tox_v3_probs_model.h5\n",
      "104s - loss: 0.0432 - acc: 0.9437 - val_loss: 0.0416 - val_acc: 0.9454\n",
      "Epoch 2/4\n",
      "Epoch 00001: val_loss did not improve\n",
      "104s - loss: 0.0404 - acc: 0.9475 - val_loss: 0.0469 - val_acc: 0.9371\n",
      "Epoch 3/4\n",
      "Epoch 00002: val_loss improved from 0.04160 to 0.03817, saving model to ../models/atn_cnn_wiki_tox_v3_probs_model.h5\n",
      "104s - loss: 0.0384 - acc: 0.9505 - val_loss: 0.0382 - val_acc: 0.9499\n",
      "Epoch 4/4\n",
      "Epoch 00003: val_loss improved from 0.03817 to 0.03717, saving model to ../models/atn_cnn_wiki_tox_v3_probs_model.h5\n",
      "104s - loss: 0.0366 - acc: 0.9525 - val_loss: 0.0372 - val_acc: 0.9513\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'atn_cnn_wiki_tox_v3'\n",
    "wiki_model = AttentionToxModel(hparams=hparams)\n",
    "wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93516059425530385"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test = pd.read_csv(wiki['test'])\n",
    "wiki_model.score_auc(wiki_test['comment'], wiki_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00000: val_loss improved from inf to 0.16953, saving model to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "103s - loss: 0.2364 - acc: 0.9184 - val_loss: 0.1695 - val_acc: 0.9387\n",
      "Epoch 2/4\n",
      "Epoch 00001: val_loss improved from 0.16953 to 0.14930, saving model to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "106s - loss: 0.1640 - acc: 0.9407 - val_loss: 0.1493 - val_acc: 0.9456\n",
      "Epoch 3/4\n",
      "Epoch 00002: val_loss improved from 0.14930 to 0.13845, saving model to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "106s - loss: 0.1460 - acc: 0.9471 - val_loss: 0.1384 - val_acc: 0.9489\n",
      "Epoch 4/4\n",
      "Epoch 00003: val_loss improved from 0.13845 to 0.13404, saving model to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "106s - loss: 0.1345 - acc: 0.9508 - val_loss: 0.1340 - val_acc: 0.9512\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'atn_cnn_debias_tox_v3'\n",
    "debias_model = ToxModel(hparams=hparams)\n",
    "debias_model.train(debias['train'], debias['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95149490074750576"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debias_test = pd.read_csv(debias['test'])\n",
    "debias_model.score_auc(debias_test['comment'], debias_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
