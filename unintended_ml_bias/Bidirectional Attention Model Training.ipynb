{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Toxicity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a model to detect toxicity in online comments. It uses a CNN architecture for text classification trained on the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) and pre-trained GloVe embeddings which can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "This model is a modification of [example code](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py) found in the [Keras Github repository](https://github.com/fchollet/keras) and released under an [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE). For further details of this license, find it [online](https://github.com/fchollet/keras/blob/master/LICENSE) or in this repository in the file KERAS_LICENSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "(TODO: nthain) - Move to README\n",
    "\n",
    "Prior to running the notebook, you must:\n",
    "\n",
    "* Download the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973)\n",
    "* Download pre-trained [GloVe embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "* (optional) To skip the training step, you will need to download a model and tokenizer file. We are looking into the appropriate means for distributing these (sometimes large) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    Tx = 250\n",
    "    repeator = RepeatVector(Tx)\n",
    "    concatenator = Concatenate(axis=-1)\n",
    "    densor1 = Dense(10, activation = \"tanh\")\n",
    "    densor2 = Dense(1, activation = \"relu\")\n",
    "    activator = Activation('softmax') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "    dotor = Dot(axes = 1)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas,a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_tool import (DEFAULT_EMBEDDINGS_PATH,\n",
    "DEFAULT_MODEL_DIR, DEFAULT_HPARAMS, ToxModel)\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Multiply\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/atn_lstm_v1\",write_graph=True)\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import cPickle\n",
    "import json\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "\n",
    "class AttentionToxModel(ToxModel):\n",
    "    def __init__(self,\n",
    "                 model_name=None,\n",
    "                 model_dir=DEFAULT_MODEL_DIR,\n",
    "                 embeddings_path=DEFAULT_EMBEDDINGS_PATH,\n",
    "                 hparams=None):\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.hparams = DEFAULT_HPARAMS.copy()\n",
    "        self.embeddings_path = embeddings_path\n",
    "        if hparams:\n",
    "            self.update_hparams(hparams)\n",
    "        if model_name:\n",
    "            self.load_model_from_name(model_name)\n",
    "            self.load_probs_model(model_name)\n",
    "        self.print_hparams()\n",
    "\n",
    "    def load_probs_model(self, model_name):\n",
    "        probs_model_name = model_name + \"_probs\"\n",
    "        self.probs_model = load_model(\n",
    "                os.path.join(\n",
    "                    self.model_dir, '%s_model.h5' % probs_model_name))\n",
    "\n",
    "    def save_prob_model(self):\n",
    "        self.probs_model_name = self.model_name + \"probs\"\n",
    "\n",
    "    def build_dense_attention_layer(self, input_tensor):\n",
    "        # softmax\n",
    "        attention_probs = Dense(self.hparams['max_sequence_length'],\n",
    "                                activation='softmax',\n",
    "                                name='attention_vec')(input_tensor)\n",
    "        # context vector\n",
    "        attention_mul = Multiply()([input_tensor, attention_probs])\n",
    "        return {'attention_probs': attention_probs,\n",
    "                'attention_preds': attention_mul}\n",
    "\n",
    "    def train(\n",
    "                self,\n",
    "                training_data_path,\n",
    "                validation_data_path, text_column,\n",
    "                label_column, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.save_hparams(model_name)\n",
    "\n",
    "        train_data = pd.read_csv(training_data_path)\n",
    "        valid_data = pd.read_csv(validation_data_path)\n",
    "\n",
    "        print('Fitting tokenizer...')\n",
    "        self.fit_and_save_tokenizer(train_data[text_column])\n",
    "        print('Tokenizer fitted!')\n",
    "\n",
    "        print('Preparing data...')\n",
    "        train_text, train_labels = (self.prep_text(train_data[text_column]),\n",
    "                                    to_categorical(train_data[label_column]))\n",
    "        valid_text, valid_labels = (self.prep_text(valid_data[text_column]),\n",
    "                                    to_categorical(valid_data[label_column]))\n",
    "        print('Data prepared!')\n",
    "\n",
    "        print('Loading embeddings...')\n",
    "        self.load_embeddings()\n",
    "        print('Embeddings loaded!')\n",
    "\n",
    "        print('Building model graph...')\n",
    "        self.build_model()\n",
    "        print('Training model...')\n",
    "\n",
    "        preds_save_path = os.path.join(\n",
    "                            self.model_dir, '%s_model.h5' % self.model_name)\n",
    "        probs_save_path = os.path.join(\n",
    "                            self.model_dir, '%s_probs_model.h5'\n",
    "                            % self.model_name)\n",
    "        preds_callbacks = [ModelCheckpoint(\n",
    "                            preds_save_path,\n",
    "                            save_best_only=True,\n",
    "                            verbose=self.hparams['verbose']), tensorboard]\n",
    "        probs_callbacks = [ModelCheckpoint(\n",
    "                            probs_save_path,\n",
    "                            save_best_only=True,\n",
    "                            verbose=self.hparams['verbose']),tensorboard]\n",
    "\n",
    "        if self.hparams['stop_early']:\n",
    "            early_stop = EarlyStopping(\n",
    "                            min_delta=self.hparams['es_min_delta'],\n",
    "                            monitor='val_loss',\n",
    "                            patience=self.hparams['es_patience'],\n",
    "                            verbose=self.hparams['verbose'], mode='auto')\n",
    "            probs_callbacks.append(early_stop)\n",
    "            preds_callbacks.append(early_stop)\n",
    "\n",
    "        self.model.fit(train_text,\n",
    "                       train_labels,\n",
    "                       batch_size=self.hparams['batch_size'],\n",
    "                       epochs=self.hparams['epochs'],\n",
    "                       validation_data=(valid_text, valid_labels),\n",
    "                       callbacks=preds_callbacks,\n",
    "                       verbose=2)\n",
    "\n",
    "        print('Model trained!')\n",
    "        print('Best model saved to {}'.format(preds_save_path))\n",
    "        print('Fitting probs model')\n",
    "\n",
    "        self.probs_model.fit(\n",
    "                    train_text,\n",
    "                    train_labels,\n",
    "                    batch_size=self.hparams['batch_size'],\n",
    "                    epochs=self.hparams['epochs'],\n",
    "                    validation_data=(valid_text, valid_labels),\n",
    "                    callbacks=probs_callbacks,\n",
    "                    verbose=2)\n",
    "        self.probs_model = load_model(probs_save_path)\n",
    "        print('Loading best model from checkpoint...')\n",
    "        self.model = load_model(preds_save_path)\n",
    "        print('Model loaded!')\n",
    "\n",
    "    def build_model(self):\n",
    "        print('print inside build model')\n",
    "        sequence_input = Input(\n",
    "                            shape=(self.hparams['max_sequence_length'],),\n",
    "                            dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "                            len(self.tokenizer.word_index) + 1,\n",
    "                            self.hparams['embedding_dim'],\n",
    "                            weights=[self.embedding_matrix],\n",
    "                            input_length=self.hparams['max_sequence_length'],\n",
    "                            trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = embedded_sequences\n",
    "    \n",
    "        # Defined shared layers as global variables\n",
    "        Tx = self.hparams['max_sequence_length']\n",
    "\n",
    "        \"\"\"\n",
    "        x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
    "        \n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(self.hparams['dropout_rate'])(x)\n",
    "        # TODO(nthain): Parametrize the number and size of fully connected layers\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        \"\"\"\n",
    "        X = x\n",
    "        n_a = 32\n",
    "        n_s = 128\n",
    "        post_activation_LSTM_cell = LSTM(128, return_state = True)\n",
    "        output_layer = Dense(self.hparams['max_sequence_length'], activation='softmax')\n",
    "        s0 = Input(shape=(n_s,), name='s0')\n",
    "        c0 = Input(shape=(n_s,), name='c0')\n",
    "        s = s0\n",
    "        c = c0\n",
    "        \n",
    "        \n",
    "         # Initialize empty list of outputs\n",
    "        outputs = []\n",
    "    \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "    \n",
    "        # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "        a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "        # Step 2: Iterate for Ty steps\n",
    "        for t in range(self.hparams['max_sequence_length']):\n",
    "    \n",
    "            # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "            context = one_step_attention(a, s)\n",
    "\n",
    "            # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "            # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "            s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
    "\n",
    "            # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "            out = output_layer(s)\n",
    "\n",
    "            # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "            outputs.append(out)\n",
    "\n",
    "            # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "        model = Model(inputs=[sequence_input,s0,c0], outputs=outputs)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.model = model\n",
    "        self.model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=rmsprop,\n",
    "                metrics=['acc'])\n",
    "        \"\"\"\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        self.model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=rmsprop,\n",
    "                metrics=['acc'])\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        for filter_size, kernel_size, pool_size in zip(\n",
    "                self.hparams['cnn_filter_sizes'],\n",
    "                self.hparams['cnn_kernel_sizes'],\n",
    "                self.hparams['cnn_pooling_sizes']):\n",
    "            x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(self.hparams['dropout_rate'], name=\"Dropout\")(x)\n",
    "        x = Dense(250, activation='relu', name=\"Dense_RELU\")(x)\n",
    "\n",
    "        # build prediction model\n",
    "        attention_dict = self.build_dense_attention_layer(x)\n",
    "        preds = attention_dict['attention_preds']\n",
    "        preds = Dense(2, name=\"preds_dense\", activation='softmax')(preds)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        self.model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=rmsprop,\n",
    "                metrics=['acc'])\n",
    "                \n",
    "                \n",
    "        # now make probs model\n",
    "        probs = attention_dict['attention_probs']\n",
    "        probs = Dense(2, name='probs_dense')(probs)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.probs_model = Model(sequence_input, preds)\n",
    "        self.probs_model.compile(\n",
    "                loss='mse', optimizer=rmsprop, metrics=['acc'])\n",
    "        # build probabilities model\n",
    "        self.save_prob_model()\n",
    "\n",
    "        \"\"\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/wiki_debias_random_%s.csv' % split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams = {'epochs': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model expects 3 input arrays, but only received one array. Found: array with shape (99157, 250)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-1f79b83a309c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'atn_lstm_v1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdebias_random_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionToxModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdebias_random_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'comment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'is_toxic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-0f0b00ef46d0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data_path, validation_data_path, text_column, label_column, model_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                        verbose=2)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model trained!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1379\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    109\u001b[0m                              \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                              \u001b[0;34m' arrays, but only received one array. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                              'Found: array with shape ' + str(data.shape))\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The model expects 3 input arrays, but only received one array. Found: array with shape (99157, 250)"
     ]
    }
   ],
   "source": [
    "# bidirectional\n",
    "MODEL_NAME = 'atn_lstm_v1'\n",
    "debias_random_model = AttentionToxModel(hparams=hparams)\n",
    "debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 5\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/5\n",
      "Epoch 00000: val_loss improved from inf to 0.17158, saving model to ../models/bd_debias_random_tox_v1_model.h5\n",
      "235s - loss: 0.2125 - acc: 0.9236 - val_loss: 0.1716 - val_acc: 0.9381\n",
      "Epoch 2/5\n",
      "Epoch 00001: val_loss improved from 0.17158 to 0.15761, saving model to ../models/bd_debias_random_tox_v1_model.h5\n",
      "234s - loss: 0.1658 - acc: 0.9387 - val_loss: 0.1576 - val_acc: 0.9422\n",
      "Epoch 3/5\n",
      "Epoch 00002: val_loss improved from 0.15761 to 0.15035, saving model to ../models/bd_debias_random_tox_v1_model.h5\n",
      "234s - loss: 0.1530 - acc: 0.9428 - val_loss: 0.1504 - val_acc: 0.9441\n",
      "Epoch 4/5\n",
      "Epoch 00003: val_loss improved from 0.15035 to 0.14325, saving model to ../models/bd_debias_random_tox_v1_model.h5\n",
      "234s - loss: 0.1453 - acc: 0.9459 - val_loss: 0.1433 - val_acc: 0.9477\n",
      "Epoch 5/5\n",
      "Epoch 00004: val_loss did not improve\n",
      "234s - loss: 0.1381 - acc: 0.9483 - val_loss: 0.1484 - val_acc: 0.9475\n",
      "Model trained!\n",
      "Best model saved to ../models/bd_debias_random_tox_v1_model.h5\n",
      "Fitting probs model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "AttentionToxModel instance has no attribute 'probs_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c0c92fc0f21c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bd_debias_random_tox_v1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdebias_random_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionToxModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdebias_random_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'comment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'is_toxic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d31fa706ffd9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data_path, validation_data_path, text_column, label_column, model_name)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting probs model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         self.probs_model.fit(\n\u001b[0m\u001b[1;32m    150\u001b[0m                     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: AttentionToxModel instance has no attribute 'probs_model'"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'bd_atn_random_tox_v1'\n",
    "debias_random_model = AttentionToxModel(hparams=hparams)\n",
    "debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23986, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "105s - loss: 0.3014 - acc: 0.9066 - val_loss: 0.2399 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23986 to 0.20081, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.2208 - acc: 0.9125 - val_loss: 0.2008 - val_acc: 0.9265\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20081 to 0.17676, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1911 - acc: 0.9314 - val_loss: 0.1768 - val_acc: 0.9384\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17676 to 0.16261, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1718 - acc: 0.9386 - val_loss: 0.1626 - val_acc: 0.9396\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16261 to 0.14827, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1571 - acc: 0.9423 - val_loss: 0.1483 - val_acc: 0.9447\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14827 to 0.14028, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1470 - acc: 0.9453 - val_loss: 0.1403 - val_acc: 0.9476\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14028 to 0.13410, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1382 - acc: 0.9487 - val_loss: 0.1341 - val_acc: 0.9493\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13410 to 0.13122, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "107s - loss: 0.1311 - acc: 0.9501 - val_loss: 0.1312 - val_acc: 0.9519\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13122 to 0.12781, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "108s - loss: 0.1256 - acc: 0.9529 - val_loss: 0.1278 - val_acc: 0.9528\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12781 to 0.12300, saving model to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "108s - loss: 0.1204 - acc: 0.9548 - val_loss: 0.1230 - val_acc: 0.9545\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "108s - loss: 0.1162 - acc: 0.9564 - val_loss: 0.1249 - val_acc: 0.9537\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "108s - loss: 0.1119 - acc: 0.9579 - val_loss: 0.1267 - val_acc: 0.9534\n",
      "Epoch 00011: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/20_atn_cnn_debias_random_tox_v3_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03382, saving model to ../models/20_atn_cnn_debias_random_tox_v3_probs_model.h5\n",
      "108s - loss: 0.0304 - acc: 0.9601 - val_loss: 0.0338 - val_acc: 0.9559\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = '20_atn_cnn_debias_random_tox_v3'\n",
    "debias_random_model = AttentionToxModel(hparams=hparams)\n",
    "debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "debias_random_model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00000: val_loss improved from inf to 0.23601, saving model to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "100s - loss: 0.3048 - acc: 0.9020 - val_loss: 0.2360 - val_acc: 0.9045\n",
      "Epoch 2/4\n",
      "Epoch 00001: val_loss improved from 0.23601 to 0.19425, saving model to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "103s - loss: 0.2152 - acc: 0.9170 - val_loss: 0.1943 - val_acc: 0.9311\n",
      "Epoch 3/4\n",
      "Epoch 00002: val_loss improved from 0.19425 to 0.17229, saving model to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "104s - loss: 0.1844 - acc: 0.9343 - val_loss: 0.1723 - val_acc: 0.9377\n",
      "Epoch 4/4\n",
      "Epoch 00003: val_loss improved from 0.17229 to 0.15996, saving model to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "104s - loss: 0.1654 - acc: 0.9394 - val_loss: 0.1600 - val_acc: 0.9409\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v3_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00000: val_loss improved from inf to 0.04160, saving model to ../models/atn_cnn_wiki_tox_v3_probs_model.h5\n",
      "104s - loss: 0.0432 - acc: 0.9437 - val_loss: 0.0416 - val_acc: 0.9454\n",
      "Epoch 2/4\n",
      "Epoch 00001: val_loss did not improve\n",
      "104s - loss: 0.0404 - acc: 0.9475 - val_loss: 0.0469 - val_acc: 0.9371\n",
      "Epoch 3/4\n",
      "Epoch 00002: val_loss improved from 0.04160 to 0.03817, saving model to ../models/atn_cnn_wiki_tox_v3_probs_model.h5\n",
      "104s - loss: 0.0384 - acc: 0.9505 - val_loss: 0.0382 - val_acc: 0.9499\n",
      "Epoch 4/4\n",
      "Epoch 00003: val_loss improved from 0.03817 to 0.03717, saving model to ../models/atn_cnn_wiki_tox_v3_probs_model.h5\n",
      "104s - loss: 0.0366 - acc: 0.9525 - val_loss: 0.0372 - val_acc: 0.9513\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'atn_cnn_wiki_tox_v3'\n",
    "wiki_model = AttentionToxModel(hparams=hparams)\n",
    "wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93516059425530385"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test = pd.read_csv(wiki['test'])\n",
    "wiki_model.score_auc(wiki_test['comment'], wiki_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00000: val_loss improved from inf to 0.16953, saving model to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "103s - loss: 0.2364 - acc: 0.9184 - val_loss: 0.1695 - val_acc: 0.9387\n",
      "Epoch 2/4\n",
      "Epoch 00001: val_loss improved from 0.16953 to 0.14930, saving model to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "106s - loss: 0.1640 - acc: 0.9407 - val_loss: 0.1493 - val_acc: 0.9456\n",
      "Epoch 3/4\n",
      "Epoch 00002: val_loss improved from 0.14930 to 0.13845, saving model to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "106s - loss: 0.1460 - acc: 0.9471 - val_loss: 0.1384 - val_acc: 0.9489\n",
      "Epoch 4/4\n",
      "Epoch 00003: val_loss improved from 0.13845 to 0.13404, saving model to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "106s - loss: 0.1345 - acc: 0.9508 - val_loss: 0.1340 - val_acc: 0.9512\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v3_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'atn_cnn_debias_tox_v3'\n",
    "debias_model = ToxModel(hparams=hparams)\n",
    "debias_model.train(debias['train'], debias['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95149490074750576"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debias_test = pd.read_csv(debias['test'])\n",
    "debias_model.score_auc(debias_test['comment'], debias_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
